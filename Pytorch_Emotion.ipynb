{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchaudio import datasets, transforms, info, load\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-X4LUrZgjhPa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "BAPVc7NCjhVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bab9b77-6cde-46c0-d4ee-3aa8e48b410f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dir = '/kaggle/input/ravdess-emotional-speech-audio'"
      ],
      "metadata": {
        "id": "tYUX30hGjhYW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "emotions = sorted(os.listdir(audio_dir))\n",
        "emotions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEnWh1ThkNdq",
        "outputId": "090c36d0-c0ab-4868-874b-a9649882fd2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Actor_01',\n",
              " 'Actor_02',\n",
              " 'Actor_03',\n",
              " 'Actor_04',\n",
              " 'Actor_05',\n",
              " 'Actor_06',\n",
              " 'Actor_07',\n",
              " 'Actor_08',\n",
              " 'Actor_09',\n",
              " 'Actor_10',\n",
              " 'Actor_11',\n",
              " 'Actor_12',\n",
              " 'Actor_13',\n",
              " 'Actor_14',\n",
              " 'Actor_15',\n",
              " 'Actor_16',\n",
              " 'Actor_17',\n",
              " 'Actor_18',\n",
              " 'Actor_19',\n",
              " 'Actor_20',\n",
              " 'Actor_21',\n",
              " 'Actor_22',\n",
              " 'Actor_23',\n",
              " 'Actor_24',\n",
              " 'audio_speech_actors_01-24']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(emotions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OPnmqOj7gx",
        "outputId": "0dbaa106-66e5-4f2b-9332-f7bc2a10e7c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_index = {lab: ind for ind, lab in enumerate(emotions)}\n",
        "label_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuqAFQeJv93j",
        "outputId": "3673d7fc-97c5-4cbf-a5c3-fc3be3faa77a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Actor_01': 0,\n",
              " 'Actor_02': 1,\n",
              " 'Actor_03': 2,\n",
              " 'Actor_04': 3,\n",
              " 'Actor_05': 4,\n",
              " 'Actor_06': 5,\n",
              " 'Actor_07': 6,\n",
              " 'Actor_08': 7,\n",
              " 'Actor_09': 8,\n",
              " 'Actor_10': 9,\n",
              " 'Actor_11': 10,\n",
              " 'Actor_12': 11,\n",
              " 'Actor_13': 12,\n",
              " 'Actor_14': 13,\n",
              " 'Actor_15': 14,\n",
              " 'Actor_16': 15,\n",
              " 'Actor_17': 16,\n",
              " 'Actor_18': 17,\n",
              " 'Actor_19': 18,\n",
              " 'Actor_20': 19,\n",
              " 'Actor_21': 20,\n",
              " 'Actor_22': 21,\n",
              " 'Actor_23': 22,\n",
              " 'Actor_24': 23,\n",
              " 'audio_speech_actors_01-24': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.MelSpectrogram(\n",
        "    sample_rate = 22050,\n",
        "    n_mels = 64\n",
        ")"
      ],
      "metadata": {
        "id": "A4UgzkjDjiJd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500"
      ],
      "metadata": {
        "id": "Ukaft_hQjhdj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAVDESSDataset(Dataset):\n",
        "    def __init__(self, audio_dir, transform=None, sample_rate=22050, max_len=500):\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.files = []\n",
        "        for root, dirs, files in os.walk(audio_dir):\n",
        "            for f in files:\n",
        "                if f.lower().endswith(\".wav\"):\n",
        "                    self.files.append(os.path.join(root, f))\n",
        "\n",
        "        if len(self.files) == 0:\n",
        "            raise ValueError(f\"В папке {audio_dir} нет файлов .wav!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            resample = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
        "            waveform = resample(waveform)\n",
        "\n",
        "        spec = self.transform(waveform) if self.transform else waveform\n",
        "\n",
        "        if spec.shape[-1] > self.max_len:\n",
        "            spec = spec[:, :, :self.max_len]\n",
        "        elif spec.shape[-1] < self.max_len:\n",
        "            spec = torch.nn.functional.pad(spec, (0, self.max_len - spec.shape[-1]))\n",
        "\n",
        "        emotion_id = int(file_name.split('-')[2]) - 1\n",
        "        return spec, emotion_id\n"
      ],
      "metadata": {
        "id": "6Kh1fLfmjhge"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = RAVDESSDataset(audio_dir=audio_dir, transform=transform, max_len=max_len)\n",
        "\n",
        "\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n"
      ],
      "metadata": {
        "id": "g3xrtYnMjhjB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, )"
      ],
      "metadata": {
        "id": "RpNDgSDqjhlu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "LkH7UYxejho6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13445672-2818-4a40-db42-02ead1b63f0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckAudio(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(CheckAudio, self).__init__()\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((8, 8))\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.second = nn.Sequential(\n",
        "            nn.Linear(64 * 8 * 8, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.second(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GuCkTQYrjhrz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CheckAudio(num_classes=8).to(device)"
      ],
      "metadata": {
        "id": "pJnjpsYJjhuS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "o1jqH7Bvmn-w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    y_pred = model(x_batch)\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  print(f'Эпоха {epoch+1}, Потери: {total_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fjEtla7mqxf",
        "outputId": "d7a6449b-7bbd-4dd3-d61c-9ec2c894289f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1, Потери: 136.8055\n",
            "Эпоха 2, Потери: 123.9639\n",
            "Эпоха 3, Потери: 117.7584\n",
            "Эпоха 4, Потери: 111.2374\n",
            "Эпоха 5, Потери: 108.0119\n",
            "Эпоха 6, Потери: 98.4688\n",
            "Эпоха 7, Потери: 98.8507\n",
            "Эпоха 8, Потери: 86.9484\n",
            "Эпоха 9, Потери: 78.2485\n",
            "Эпоха 10, Потери: 68.9729\n",
            "Эпоха 11, Потери: 80.6332\n",
            "Эпоха 12, Потери: 72.0293\n",
            "Эпоха 13, Потери: 59.5840\n",
            "Эпоха 14, Потери: 54.4586\n",
            "Эпоха 15, Потери: 50.3794\n",
            "Эпоха 16, Потери: 44.0706\n",
            "Эпоха 17, Потери: 39.1126\n",
            "Эпоха 18, Потери: 52.4071\n",
            "Эпоха 19, Потери: 37.5381\n",
            "Эпоха 20, Потери: 29.7961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in test_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "        pred = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "        total += y_batch.size(0)\n",
        "        correct += (pred == y_batch).sum().item()\n",
        "\n",
        "accuracy = correct * 100 / total\n",
        "print(f'точность модели : {accuracy :.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9rtpCCVm_rb",
        "outputId": "4d6c399f-fe34-4593-89a4-79faa86d396d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "точность модели : 76.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = {\n",
        "    1: \"Neutral\",\n",
        "    2: \"Calm\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Angry\",\n",
        "    6: \"Fearful\",\n",
        "    7: \"Disgust\",\n",
        "    8: \"Surprised\"\n",
        "}"
      ],
      "metadata": {
        "id": "jEfXUQ6DU0MF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = sorted(emotion_labels.values())\n",
        "print(\"Эмоции:\", emotions)\n",
        "torch.save(emotions, \"labels_emotion.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRi5IsQVU4Qb",
        "outputId": "a0a11580-09b4-4934-fa66-64d193679fd2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эмоции: ['Angry', 'Calm', 'Disgust', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_emotion.pth')"
      ],
      "metadata": {
        "id": "gTwtjbRcqHE-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBVJUNMUUzvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}